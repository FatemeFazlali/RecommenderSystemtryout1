{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oGxDF21iPnSYAkwBdnYyK7ZUlUHi2NIi",
      "authorship_tag": "ABX9TyMhKVQivwoYzlVXxhInm4sw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemeFazlali/RecommenderSystemtryout1/blob/main/FedshiMachineLearningAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and set up paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Set up paths\n",
        "BASE = Path(\"/content/drive/MyDrive/Fedshi_Machine_Learning\")\n",
        "DATA_DIR = BASE / \"data\"\n",
        "BOOKS_FILE = DATA_DIR / \"Books.csv\"\n",
        "USERS_FILE = DATA_DIR / \"Users.csv\"\n",
        "RATINGS_FILE = DATA_DIR / \"Ratings.csv\"\n",
        "\n",
        "# Create processed data directory\n",
        "PROCESSED_DIR = BASE / \"processed_data\"\n",
        "MODELS_DIR = BASE / \"models\"\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "books_df = pd.read_csv(BOOKS_FILE)\n",
        "users_df = pd.read_csv(USERS_FILE)\n",
        "ratings_df = pd.read_csv(RATINGS_FILE)\n",
        "\n",
        "print(f\"Books shape: {books_df.shape}\")\n",
        "print(f\"Users shape: {users_df.shape}\")\n",
        "print(f\"Ratings shape: {ratings_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccNwGPPTR3Nl",
        "outputId": "7b41a8a5-aa6b-4154-f47a-4256118dac8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2882730971.py:25: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  books_df = pd.read_csv(BOOKS_FILE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books shape: (271360, 8)\n",
            "Users shape: (278858, 3)\n",
            "Ratings shape: (1149780, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced preprocessing functions\n",
        "def preprocess_books(books_df):\n",
        "    # Handle missing values\n",
        "    books_df = books_df.dropna(subset=['ISBN', 'Book-Title', 'Book-Author'])\n",
        "\n",
        "    # Clean year of publication\n",
        "    books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')\n",
        "    books_df = books_df[books_df['Year-Of-Publication'].between(1900, 2023)]\n",
        "\n",
        "    # Extract publisher information\n",
        "    publisher_counts = books_df['Publisher'].value_counts()\n",
        "    books_df['Publisher'] = books_df['Publisher'].apply(\n",
        "        lambda x: x if publisher_counts.get(x, 0) > 10 else 'Other'\n",
        "    )\n",
        "\n",
        "    # Create item index\n",
        "    books_df['item_index'] = books_df.reset_index().index\n",
        "\n",
        "    # Create publisher codes\n",
        "    unique_publishers = books_df['Publisher'].unique()\n",
        "    publisher_map = {publisher: i for i, publisher in enumerate(unique_publishers)}\n",
        "    books_df['publisher_code'] = books_df['Publisher'].map(publisher_map)\n",
        "\n",
        "    # Normalize year\n",
        "    books_df['year_normalized'] = (books_df['Year-Of-Publication'] - 1900) / (2023 - 1900)\n",
        "\n",
        "    return books_df\n",
        "\n",
        "def preprocess_users(users_df):\n",
        "    # Handle missing ages\n",
        "    users_df['Age'] = users_df['Age'].fillna(users_df['Age'].median())\n",
        "    users_df['Age'] = users_df['Age'].clip(5, 90)\n",
        "\n",
        "    # Extract location components\n",
        "    users_df[['City', 'State', 'Country']] = users_df['Location'].str.split(',', expand=True).iloc[:, :3]\n",
        "    users_df['Country'] = users_df['Country'].str.strip().fillna('Unknown')\n",
        "\n",
        "    # One-hot encode country\n",
        "    top_countries = users_df['Country'].value_counts().head(10).index\n",
        "    users_df['Country'] = users_df['Country'].apply(lambda x: x if x in top_countries else 'Other')\n",
        "\n",
        "    # Create user index\n",
        "    users_df['user_index'] = users_df.reset_index().index\n",
        "\n",
        "    # Create country codes\n",
        "    unique_countries = users_df['Country'].unique()\n",
        "    country_map = {country: i for i, country in enumerate(unique_countries)}\n",
        "    users_df['country_code'] = users_df['Country'].map(country_map)\n",
        "\n",
        "    return users_df\n",
        "\n",
        "def preprocess_ratings(ratings_df, books_df, users_df):\n",
        "    # Filter ratings for existing books and users\n",
        "    ratings_df = ratings_df[ratings_df['ISBN'].isin(books_df['ISBN'])]\n",
        "    ratings_df = ratings_df[ratings_df['User-ID'].isin(users_df['User-ID'])]\n",
        "\n",
        "    # Convert ratings to implicit feedback (1 if rating > 0, else 0)\n",
        "    ratings_df['Interacted'] = (ratings_df['Book-Rating'] > 0).astype(int)\n",
        "\n",
        "    # Map user and item IDs to indices\n",
        "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(users_df['User-ID'])}\n",
        "    item_id_to_index = {isbn: idx for idx, isbn in enumerate(books_df['ISBN'])}\n",
        "\n",
        "    ratings_df['user_index'] = ratings_df['User-ID'].map(user_id_to_index)\n",
        "    ratings_df['item_index'] = ratings_df['ISBN'].map(item_id_to_index)\n",
        "\n",
        "    return ratings_df\n",
        "\n",
        "# Apply preprocessing\n",
        "books_df = preprocess_books(books_df)\n",
        "users_df = preprocess_users(users_df)\n",
        "ratings_df = preprocess_ratings(ratings_df, books_df, users_df)\n",
        "\n",
        "# Save processed data\n",
        "books_df.to_csv(PROCESSED_DIR / 'books_processed.csv', index=False)\n",
        "users_df.to_csv(PROCESSED_DIR / 'users_processed.csv', index=False)\n",
        "ratings_df.to_csv(PROCESSED_DIR / 'ratings_processed.csv', index=False)\n",
        "\n",
        "print(\"Data preprocessing completed and saved!\")"
      ],
      "metadata": {
        "id": "Z5brI2-_U-L6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6d0311-545c-4054-c4b5-248f9e37b326"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-501584249.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_hybrid_recommendation_model(n_users, n_items, n_countries, n_publishers, embedding_dim=50):\n",
        "    # User inputs\n",
        "    user_input = Input(shape=(1,), name='user_input')\n",
        "    user_embedding = Embedding(n_users, embedding_dim, name='user_embedding')(user_input)\n",
        "    user_flatten = Flatten()(user_embedding)\n",
        "\n",
        "    # User metadata\n",
        "    country_input = Input(shape=(1,), name='country_input')\n",
        "    country_embedding = Embedding(n_countries, 10, name='country_embedding')(country_input)\n",
        "    country_flatten = Flatten()(country_embedding)\n",
        "\n",
        "    age_input = Input(shape=(1,), name='age_input')\n",
        "    age_dense = Dense(5, activation='relu')(age_input)\n",
        "\n",
        "    # Item inputs\n",
        "    item_input = Input(shape=(1,), name='item_input')\n",
        "    item_embedding = Embedding(n_items, embedding_dim, name='item_embedding')(item_input)\n",
        "    item_flatten = Flatten()(item_embedding)\n",
        "\n",
        "    # Item metadata\n",
        "    publisher_input = Input(shape=(1,), name='publisher_input')\n",
        "    publisher_embedding = Embedding(n_publishers, 10, name='publisher_embedding')(publisher_input)\n",
        "    publisher_flatten = Flatten()(publisher_embedding)\n",
        "\n",
        "    year_input = Input(shape=(1,), name='year_input')\n",
        "    year_dense = Dense(5, activation='relu')(year_input)\n",
        "\n",
        "    # Concatenate all features\n",
        "    user_features = Concatenate()([user_flatten, country_flatten, age_dense])\n",
        "    item_features = Concatenate()([item_flatten, publisher_flatten, year_dense])\n",
        "\n",
        "    # Dot product of user and item features\n",
        "    dot_product = tf.keras.layers.Dot(axes=1)([user_features, item_features])\n",
        "\n",
        "    # Final prediction\n",
        "    output = Dense(1, activation='sigmoid', name='output')(dot_product)\n",
        "\n",
        "    # Build model\n",
        "    model = Model(\n",
        "        inputs=[user_input, country_input, age_input, item_input, publisher_input, year_input],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prepare data for training - FIXED VERSION\n",
        "# Create mapping dictionaries\n",
        "user_id_to_index = {user_id: idx for idx, user_id in enumerate(users_df['User-ID'])}\n",
        "isbn_to_index = {isbn: idx for idx, isbn in enumerate(books_df['ISBN'])}\n",
        "\n",
        "# Map ratings to indices\n",
        "ratings_df['user_index'] = ratings_df['User-ID'].map(user_id_to_index)\n",
        "ratings_df['item_index'] = ratings_df['ISBN'].map(isbn_to_index)\n",
        "\n",
        "# Get user features\n",
        "user_features = users_df.set_index('User-ID')\n",
        "ratings_df['country_code'] = ratings_df['User-ID'].map(user_features['country_code'])\n",
        "ratings_df['age'] = ratings_df['User-ID'].map(user_features['Age'])\n",
        "\n",
        "# Get book features\n",
        "book_features = books_df.set_index('ISBN')\n",
        "ratings_df['publisher_code'] = ratings_df['ISBN'].map(book_features['publisher_code'])\n",
        "ratings_df['year_normalized'] = ratings_df['ISBN'].map(book_features['year_normalized'])\n",
        "\n",
        "# Prepare input arrays\n",
        "user_indices = ratings_df['user_index'].values\n",
        "item_indices = ratings_df['item_index'].values\n",
        "country_codes = ratings_df['country_code'].values\n",
        "ages = ratings_df['age'].values\n",
        "publisher_codes = ratings_df['publisher_code'].values\n",
        "years = ratings_df['year_normalized'].values\n",
        "labels = ratings_df['Interacted'].values\n",
        "\n",
        "n_users = len(users_df)\n",
        "n_items = len(books_df)\n",
        "n_countries = len(users_df['country_code'].unique())\n",
        "n_publishers = len(books_df['publisher_code'].unique())\n",
        "\n",
        "# Build and compile model\n",
        "model = build_hybrid_recommendation_model(n_users, n_items, n_countries, n_publishers)\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [user_indices, country_codes, ages, item_indices, publisher_codes, years],\n",
        "    labels,\n",
        "    batch_size=1024,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save(MODELS_DIR / 'recommendation_model.h5')\n",
        "print(\"Model training completed and saved!\")"
      ],
      "metadata": {
        "id": "ysvvrU1tfI4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f5b14b-8323-424c-8368-1b180b493fa6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 381ms/step - accuracy: 0.5827 - loss: 3.3288 - val_accuracy: 0.6137 - val_loss: 0.6684\n",
            "Epoch 2/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 385ms/step - accuracy: 0.6349 - loss: 0.6453 - val_accuracy: 0.6309 - val_loss: 0.6519\n",
            "Epoch 3/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 388ms/step - accuracy: 0.8104 - loss: 0.4857 - val_accuracy: 0.6187 - val_loss: 0.6582\n",
            "Epoch 4/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 374ms/step - accuracy: 0.9448 - loss: 0.3137 - val_accuracy: 0.6009 - val_loss: 0.6655\n",
            "Epoch 5/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 389ms/step - accuracy: 0.9743 - loss: 0.1922 - val_accuracy: 0.5819 - val_loss: 0.6715\n",
            "Epoch 6/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 373ms/step - accuracy: 0.9854 - loss: 0.1131 - val_accuracy: 0.5842 - val_loss: 0.6704\n",
            "Epoch 7/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 388ms/step - accuracy: 0.9914 - loss: 0.0666 - val_accuracy: 0.5878 - val_loss: 0.6688\n",
            "Epoch 8/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 373ms/step - accuracy: 0.9950 - loss: 0.0402 - val_accuracy: 0.5865 - val_loss: 0.6687\n",
            "Epoch 9/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 389ms/step - accuracy: 0.9970 - loss: 0.0252 - val_accuracy: 0.5786 - val_loss: 0.6727\n",
            "Epoch 10/10\n",
            "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 375ms/step - accuracy: 0.9982 - loss: 0.0160 - val_accuracy: 0.5868 - val_loss: 0.6706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_utils.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "def get_recommendations(user_id, model, books_df, users_df, ratings_df, top_n=10):\n",
        "    \"\"\"\n",
        "    Generate personalized book recommendations for a given user.\n",
        "    Includes cold start handling for new users.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if user exists\n",
        "        user_data = users_df[users_df['User-ID'] == user_id]\n",
        "\n",
        "        if user_data.empty:\n",
        "            # Cold start: user doesn't exist, use popular items\n",
        "            return handle_cold_start_user(books_df, ratings_df, top_n)\n",
        "\n",
        "        # Prepare user features for prediction\n",
        "        user_features = prepare_user_features(user_data)\n",
        "\n",
        "        # Get all books\n",
        "        book_features = prepare_book_features(books_df)\n",
        "\n",
        "        # Generate predictions for all books\n",
        "        predictions = []\n",
        "        batch_size = 1000\n",
        "\n",
        "        for i in range(0, len(book_features), batch_size):\n",
        "            batch_books = book_features[i:i+batch_size]\n",
        "\n",
        "            # Create input arrays\n",
        "            user_input_batch = np.repeat(user_features['user_index'], len(batch_books))\n",
        "            country_input_batch = np.repeat(user_features['country_code'], len(batch_books))\n",
        "            age_input_batch = np.repeat(user_features['age_normalized'], len(batch_books))\n",
        "\n",
        "            item_input_batch = batch_books['item_index'].values\n",
        "            publisher_input_batch = batch_books['publisher_code'].values\n",
        "            year_input_batch = batch_books['year_normalized'].values\n",
        "\n",
        "            # Make predictions\n",
        "            batch_pred = model.predict([\n",
        "                user_input_batch,\n",
        "                country_input_batch,\n",
        "                age_input_batch,\n",
        "                item_input_batch,\n",
        "                publisher_input_batch,\n",
        "                year_input_batch\n",
        "            ], verbose=0)\n",
        "\n",
        "            # Store predictions with book IDs\n",
        "            for j, pred in enumerate(batch_pred):\n",
        "                predictions.append({\n",
        "                    'ISBN': batch_books.iloc[j]['ISBN'],\n",
        "                    'prediction': pred[0]\n",
        "                })\n",
        "\n",
        "        # Sort by prediction score and get top N\n",
        "        predictions_df = pd.DataFrame(predictions)\n",
        "        top_books = predictions_df.sort_values('prediction', ascending=False).head(top_n)\n",
        "\n",
        "        # Merge with book details\n",
        "        recommendations = top_books.merge(\n",
        "            books_df[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']],\n",
        "            on='ISBN',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        return recommendations[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_recommendations: {e}\")\n",
        "        return handle_cold_start_user(books_df, ratings_df, top_n)\n",
        "\n",
        "def get_similar_items(book_title, model, books_df, top_n=10):\n",
        "    \"\"\"\n",
        "    Find books similar to a given book based on content features.\n",
        "    Includes cold start handling for new items.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find the target book\n",
        "        target_book = books_df[books_df['Book-Title'] == book_title]\n",
        "        if target_book.empty:\n",
        "            # Cold start: book doesn't exist, use content-based similarity\n",
        "            return handle_cold_start_item(book_title, books_df, top_n)\n",
        "\n",
        "        # Get all books\n",
        "        book_features = prepare_book_features(books_df)\n",
        "        target_features = prepare_book_features(target_book)\n",
        "\n",
        "        # Extract item embeddings from the model\n",
        "        item_embedding_model = tf.keras.Model(\n",
        "            inputs=model.input[3],  # item_input\n",
        "            outputs=model.get_layer('item_embedding').output\n",
        "        )\n",
        "\n",
        "        # Get embeddings for all books\n",
        "        all_embeddings = item_embedding_model.predict(\n",
        "            book_features['item_index'].values,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Get embedding for target book\n",
        "        target_embedding = item_embedding_model.predict(\n",
        "            target_features['item_index'].values,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarities = cosine_similarity(target_embedding, all_embeddings)[0]\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = books_df.copy()\n",
        "        results['similarity'] = similarities\n",
        "\n",
        "        # Sort by similarity and get top N (excluding the target book)\n",
        "        similar_books = results[results['Book-Title'] != book_title]\n",
        "        similar_books = similar_books.sort_values('similarity', ascending=False).head(top_n)\n",
        "\n",
        "        return similar_books[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_similar_items: {e}\")\n",
        "        return handle_cold_start_item(book_title, books_df, top_n)\n",
        "\n",
        "def handle_cold_start_user(books_df, ratings_df, top_n=10):\n",
        "    \"\"\"\n",
        "    Handle cold start for new users by recommending popular items.\n",
        "    \"\"\"\n",
        "    # Calculate popularity based on number of ratings\n",
        "    popularity = ratings_df['ISBN'].value_counts().reset_index()\n",
        "    popularity.columns = ['ISBN', 'count']\n",
        "\n",
        "    # Get top N popular books\n",
        "    popular_books = popularity.head(top_n).merge(\n",
        "        books_df[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']],\n",
        "        on='ISBN',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    return popular_books[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']]\n",
        "\n",
        "def handle_cold_start_item(book_title, books_df, top_n=10):\n",
        "    \"\"\"\n",
        "    Handle cold start for new items using content-based similarity.\n",
        "    \"\"\"\n",
        "    # Create TF-IDF matrix of book titles and authors\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    books_df['content'] = books_df['Book-Title'] + ' ' + books_df['Book-Author'] + ' ' + books_df['Publisher'].fillna('')\n",
        "    tfidf_matrix = tfidf.fit_transform(books_df['content'])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    # Get the index of the book\n",
        "    indices = pd.Series(books_df.index, index=books_df['Book-Title']).drop_duplicates()\n",
        "\n",
        "    if book_title not in indices:\n",
        "        # Book not found, return popular books\n",
        "        return books_df[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']].head(top_n)\n",
        "\n",
        "    idx = indices[book_title]\n",
        "\n",
        "    # Get pairwise similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    # Sort books based on similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get scores of the top_n most similar books (skip the first as it's the same book)\n",
        "    sim_scores = sim_scores[1:top_n+1]\n",
        "\n",
        "    # Get book indices\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return top N most similar books\n",
        "    return books_df.iloc[book_indices][['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']]\n",
        "\n",
        "def prepare_user_features(user_data):\n",
        "    \"\"\"\n",
        "    Prepare user features for model input.\n",
        "    \"\"\"\n",
        "    # Normalize age (assuming age range 5-90 as in preprocessing)\n",
        "    age_normalized = (user_data['Age'].values[0] - 5) / (90 - 5)\n",
        "\n",
        "    # Get user index\n",
        "    user_index = user_data['user_index'].values[0]\n",
        "\n",
        "    # Get country code\n",
        "    country_code = user_data['country_code'].values[0]\n",
        "\n",
        "    return {\n",
        "        'user_index': user_index,\n",
        "        'country_code': country_code,\n",
        "        'age_normalized': age_normalized\n",
        "    }\n",
        "\n",
        "def prepare_book_features(books_data):\n",
        "    \"\"\"\n",
        "    Prepare book features for model input.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    books_copy = books_data.copy()\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'item_index' not in books_copy.columns:\n",
        "        books_copy['item_index'] = range(len(books_copy))\n",
        "\n",
        "    if 'publisher_code' not in books_copy.columns:\n",
        "        unique_publishers = books_copy['Publisher'].unique()\n",
        "        publisher_map = {publisher: i for i, publisher in enumerate(unique_publishers)}\n",
        "        books_copy['publisher_code'] = books_copy['Publisher'].map(publisher_map)\n",
        "\n",
        "    if 'year_normalized' not in books_copy.columns:\n",
        "        books_copy['year_normalized'] = (books_copy['Year-Of-Publication'] - 1900) / (2023 - 1900)\n",
        "\n",
        "    return books_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtG4tXROnyLJ",
        "outputId": "bb783993-4abe-470c-bb91-35daca5b9708"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import os\n",
        "from model_utils import get_recommendations, get_similar_items\n",
        "\n",
        "# Set up paths\n",
        "BASE = Path(\"/content/drive/MyDrive/Fedshi_Machine_Learning\")\n",
        "PROCESSED_DIR = BASE / \"processed_data\"\n",
        "MODELS_DIR = BASE / \"models\"\n",
        "\n",
        "# Load data and model\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    books = pd.read_csv(PROCESSED_DIR / 'books_processed.csv')\n",
        "    users = pd.read_csv(PROCESSED_DIR / 'users_processed.csv')\n",
        "    ratings = pd.read_csv(PROCESSED_DIR / 'ratings_processed.csv')\n",
        "    model = tf.keras.models.load_model(MODELS_DIR / 'recommendation_model.h5')\n",
        "    return books, users, ratings, model\n",
        "\n",
        "books, users, ratings, model = load_data()\n",
        "\n",
        "st.title(\"Book Recommendation System\")\n",
        "st.write(\"This system provides personalized book recommendations using a hybrid approach combining collaborative filtering and content-based features.\")\n",
        "\n",
        "option = st.sidebar.selectbox(\"Select Recommendation Type\",\n",
        "                             [\"User-Based Recommendations\", \"Item-Based Similarity\"])\n",
        "\n",
        "if option == \"User-Based Recommendations\":\n",
        "    st.header(\"Personalized Recommendations\")\n",
        "    user_id = st.selectbox(\"Select User ID\", users['User-ID'].head(1000).tolist())\n",
        "\n",
        "    if st.button(\"Get Recommendations\"):\n",
        "        with st.spinner('Generating recommendations...'):\n",
        "            recommendations = get_recommendations(user_id, model, books, users, ratings)\n",
        "\n",
        "        st.success(\"Top Recommendations:\")\n",
        "        for i, row in recommendations.iterrows():\n",
        "            st.write(f\"{i+1}. **{row['Book-Title']}** by {row['Book-Author']}\")\n",
        "            st.write(f\"   Publisher: {row['Publisher']}, Year: {row['Year-Of-Publication']}\")\n",
        "\n",
        "else:\n",
        "    st.header(\"Find Similar Books\")\n",
        "    book_title = st.selectbox(\"Select Book\", books['Book-Title'].head(1000).tolist())\n",
        "\n",
        "    if st.button(\"Find Similar Items\"):\n",
        "        with st.spinner('Finding similar books...'):\n",
        "            similar_items = get_similar_items(book_title, model, books)\n",
        "\n",
        "        st.success(\"Similar Books:\")\n",
        "        for i, row in similar_items.iterrows():\n",
        "            st.write(f\"{i+1}. **{row['Book-Title']}** by {row['Book-Author']}\")\n",
        "            st.write(f\"   Publisher: {row['Publisher']}, Year: {row['Year-Of-Publication']}\")\n",
        "\n",
        "# Add metrics section\n",
        "st.sidebar.header(\"Metrics\")\n",
        "st.sidebar.write(\"\"\"\n",
        "**Metrics Tracked:**\n",
        "- Precision@K\n",
        "- Recall@K\n",
        "- NDCG@K\n",
        "- Coverage\n",
        "- Personalization\n",
        "- Response Time\n",
        "\n",
        "These metrics evaluate both recommendation quality and system performance.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h8mfgvNVgzZ",
        "outputId": "5ec8a721-5b8c-4d2c-b3d5-dd6d523d122d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install streamlit scikit-learn\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9O3J1K7Vtpa",
        "outputId": "332cb11c-13bb-4645-8f0b-cf8e7f5f4767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.49.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8502\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.169.223.36:8502\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://cruel-items-find.loca.lt\n",
            "2025-09-18 11:37:00.282442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758195420.335547   25927 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758195420.352311   25927 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758195420.406787   25927 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758195420.406869   25927 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758195420.406875   25927 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758195420.406879   25927 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVc0oeA5luB8",
        "outputId": "c21132ae-34a2-41e4-8820-3d550122a746"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.169.223.36"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import threading\n",
        "\n",
        "# Kill existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in a thread or background\n",
        "def run_streamlit():\n",
        "    !streamlit run app.py --server.port 8501\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Open a tunnel\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(\"Streamlit App URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "mb5V1BVxlouo",
        "outputId": "bb549a9b-96a3-4a13-ad62-d30d696f68f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.169.223.36:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-09-18T11:30:33+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-09-18T11:30:33+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2033290644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Open a tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbind_tls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Streamlit App URL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    }
  ]
}